{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from llm.factory import LLMInterface\n",
    "from llm.embedding import get_text_embedding\n",
    "from setting.db import db_manager\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "from knowledge_graph.graph_builder import KnowledgeGraphBuilder\n",
    "\n",
    "llm_client = LLMInterface(\"ollama\", \"qwen3:32b-fp16\")\n",
    "session_factory = db_manager.get_session_factory(os.getenv(\"GRAPH_DATABASE_URI\"))\n",
    "kb_builder = KnowledgeBuilder(session_factory)\n",
    "graph_builder = KnowledgeGraphBuilder(llm_client, get_text_embedding, session_factory)\n",
    "\n",
    "# Initialize logging module with a basic configuration for console output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s - %(filename)s:%(lineno)d: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "categories = [\n",
    "\"tidb/About TiDB Self-Managed\",\n",
    "\"tidb/Get Started\",\n",
    "\"tidb/Develop/overview\",\n",
    "\"tidb/Develop/Quick Start\",\n",
    "\"tidb/Develop/Example Applications\",\n",
    "\"tidb/Develop/Connect to TiDB\",\n",
    "\"tidb/Develop/Design Database Schema\",\n",
    "\"tidb/Develop/Write Data\",\n",
    "\"tidb/Develop/Read Data\",\n",
    "\"tidb/Develop/Vector Search\",\n",
    "\"tidb/Develop/Transaction\",\n",
    "\"tidb/Develop/Optimize\",\n",
    "\"tidb/Develop/Troubleshoot\",\n",
    "\"tidb/Develop/Reference\",\n",
    "\"tidb/Develop/Cloud Native Development Environment\",\n",
    "\"tidb/Develop/Third-Party Support\",\n",
    "\"tidb/Deploy\",\n",
    "\"tidb/Migrate\",\n",
    "\"tidb/Stream Data\",\n",
    "\"tidb/Maintain/Security\",\n",
    "\"tidb/Maintain/Upgrade\",\n",
    "\"tidb/Maintain/Scale\",\n",
    "\"tidb/Maintain/Backup and Restore\",\n",
    "\"tidb/Maintain/Cluster Disaster Recovery (DR)\",\n",
    "\"tidb/Maintain/Resource Manager\",\n",
    "\"tidb/Maintain/Configure Time Zone\",\n",
    "\"tidb/Maintain/Daily Checklist\",\n",
    "\"tidb/Maintain/Maintain TiFlash\",\n",
    "\"tidb/Maintain/Maintain TiDB Using TiUP\",\n",
    "\"tidb/Maintain/Modify Configuration Dynamically\",\n",
    "\"tidb/Maintain/Online Unsafe Recovery\",\n",
    "\"tidb/Maintain/Replicate Data Between Primary and Secondary Clusters\",\n",
    "\"tidb/Monitor and Alert\",\n",
    "\"tidb/Troubleshoot\",\n",
    "\"tidb/Performance Tuning\",\n",
    "\"tidb/Tutorials\",\n",
    "\"tidb/TiDB Tools/overview\",\n",
    "\"tidb/TiDB Tools/TiUP\",\n",
    "\"tidb/TiDB Tools/TiDB Operator\",\n",
    "\"tidb/TiDB Tools/TiDB Data Migration\",\n",
    "\"tidb/TiDB Tools/TiDB Lightning\",\n",
    "\"tidb/TiDB Tools/Dumpling\",\n",
    "\"tidb/TiDB Tools/PingCAP Clinic Diagnostic Service\",\n",
    "\"tidb/TiDB Tools/TiSpark\",\n",
    "\"tidb/TiDB Tools/sync-diff-inspector\",\n",
    "\"tidb/TiDB Tools/TiProxy\",\n",
    "\"tidb/Reference/Cluster Architecture\",\n",
    "\"tidb/Reference/Storage Engine - TiKV\",\n",
    "\"tidb/Reference/Storage Engine - TiFlash\",\n",
    "\"tidb/Reference/TiDB Distributed eXecution Framework (DXF)\",\n",
    "\"tidb/Reference/System Variables\",\n",
    "\"tidb/Reference/Configuration File Parameters\",\n",
    "\"tidb/Reference/CLI\",\n",
    "\"tidb/Reference/Command Line Flags\",\n",
    "\"tidb/Reference/Key Monitoring Metrics\",\n",
    "\"tidb/Reference/Privileges\",\n",
    "\"tidb/Reference/SQL\",\n",
    "\"tidb/Reference/Telemetry\",\n",
    "\"tidb/Reference/Error Codes\",\n",
    "\"tidb/Reference/Table Filter\",\n",
    "\"tidb/Reference/Schedule Replicas by Topology Labels\",\n",
    "\"tidb/Reference/URI Formats of External Storage Services\",\n",
    "\"tidb/FAQs\",\n",
    "\"tidb/Release Notes\",\n",
    "\"tidb/Glossary\"\n",
    "]\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = '/Users/ian/Work/docs/toc_files.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")\n",
    "\n",
    "\n",
    "tidb_product_docs = {}\n",
    "for category in categories:\n",
    "    if category == \"tidb/Reference/SQL\":\n",
    "        continue\n",
    "    topic_name = \"TiDB Product Documentation - \" + category\n",
    "    tidb_product_docs[topic_name] = []\n",
    "    for doc in loaded_docs:\n",
    "        if category in doc['category']:\n",
    "            tidb_product_docs[topic_name].append({\n",
    "                'topic_name': topic_name,\n",
    "                'path': doc['path'],  # required\n",
    "                'doc_link': doc['web_view_link'], # required\n",
    "                'category': category,\n",
    "                'updated_at': doc['modified_time'],\n",
    "                'mime_type': doc['mime_type'],\n",
    "                'version': doc['version']\n",
    "            })\n",
    "    print(f\"Category: {topic_name}, Number of documents: {len(tidb_product_docs[topic_name])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"step 1: upload docs to knowledge base\")\n",
    "topic_docs = {}\n",
    "for topic_name in tidb_product_docs:\n",
    "    print(\"uploading docs for topic: \", topic_name)\n",
    "    docs = tidb_product_docs[topic_name]\n",
    "    uploaded_docs = {}\n",
    "    for doc in docs:\n",
    "        file_path = doc['path']\n",
    "        try:\n",
    "            res = kb_builder.extract_knowledge(\n",
    "                file_path, \n",
    "                doc\n",
    "            )\n",
    "            if res['status'] == 'success':\n",
    "                uploaded_docs[res['source_id']] = {\n",
    "                    \"source_id\": res['source_id'],\n",
    "                    \"source_name\": res['source_name'],\n",
    "                    \"source_content\": res['source_content'],\n",
    "                    \"source_link\": res['source_link'],\n",
    "                    \"topic_name\": topic_name,\n",
    "                    \"source_attributes\": res['source_attributes']\n",
    "                } \n",
    "            else:\n",
    "                print(f\"process index {file_path} failed, {res['error']}\", exc_info=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"process index {file_path} failed, {e}\", exc_info=True)\n",
    "    \n",
    "    topic_docs[topic_name] = list(uploaded_docs.values())\n",
    "\n",
    "topic_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Build Graph \n",
    "\n",
    "Assuming that source data already uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from knowledge_graph.models import SourceData\n",
    "from setting.db import db_manager\n",
    "\n",
    "def get_documents_by_topic(database_uri):\n",
    "    \"\"\"\n",
    "    Query database to get all documents aggregated by topic_name.\n",
    "    Returns a dict where each topic has an array of documents.\n",
    "    \"\"\"\n",
    "    topic_docs = defaultdict(list)\n",
    "    session_factory = db_manager.get_session_factory(database_uri)\n",
    "    with session_factory() as db:\n",
    "        # Query all source data\n",
    "        source_data_list = db.query(SourceData).all()\n",
    "        \n",
    "        for source_data in source_data_list:\n",
    "            # Extract topic_name from attributes\n",
    "            topic_name = None\n",
    "            if source_data.attributes and isinstance(source_data.attributes, dict):\n",
    "                topic_name = source_data.attributes.get('topic_name')\n",
    "            \n",
    "            # Skip if no topic_name found\n",
    "            if not topic_name:\n",
    "                continue\n",
    "                \n",
    "            # Add document to the appropriate topic\n",
    "            doc_info = {\n",
    "                \"source_id\": source_data.id,\n",
    "                \"source_name\": source_data.name,\n",
    "                \"source_content\": source_data.effective_content,\n",
    "                \"source_link\": source_data.link,\n",
    "                \"topic_name\": topic_name,\n",
    "                \"source_attributes\": source_data.attributes,\n",
    "            }\n",
    "            topic_docs[topic_name].append(doc_info)\n",
    "    \n",
    "    return dict(topic_docs)\n",
    "\n",
    "# Get all documents grouped by topic\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "all_topic_docs = get_documents_by_topic(database_uri)\n",
    "\n",
    "# Display available topics\n",
    "print(\"Available topics:\")\n",
    "for topic, docs in all_topic_docs.items():\n",
    "    print(f\"  {topic}: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = list(all_topic_docs.keys())\n",
    "topic_names = sorted(topic_names)\n",
    "for i, topic_name in enumerate(topic_names):\n",
    "    if i % 6 != 4:\n",
    "        continue\n",
    "    topic_docs = all_topic_docs[topic_name]\n",
    "    logger.info(\"processing topic: %s, number of docs: %d\", topic_name, len(topic_docs))\n",
    "    try:\n",
    "        result = graph_builder.build_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\n=== Memory Knowledge Graph Construction Results ===\")\n",
    "        logger.info(f\"Topic: {result['topic_name']}\")\n",
    "        logger.info(f\"Documents processed: {result['documents_processed']}\")\n",
    "        logger.info(f\"Documents failed: {result['documents_failed']}\")\n",
    "        logger.info(f\"Cognitive maps generated: {result['cognitive_maps_generated']}\")\n",
    "        logger.info(f\"Triplets extracted: {result['triplets_extracted']}\")\n",
    "        logger.info(f\"Total entities created: {result['entities_created']}\")\n",
    "        logger.info(f\"Total relationships created: {result['relationships_created']}\")\n",
    "\n",
    "        # Print global blueprint information\n",
    "        blueprint_info = result.get(\"global_blueprint\", {})\n",
    "        logger.info(f\"\\nGlobal Blueprint:\")\n",
    "        logger.info(\n",
    "            f\"  - Processing instructions: {blueprint_info.get('processing_instructions', '')}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"  - Processing items: {blueprint_info.get('processing_items', {})}\"\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\nðŸŽ‰ Memory knowledge graph construction completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build knowledge graph: {e}\", exc_info=True)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        result = graph_builder.enhance_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ehance knowledge graph: {e}\", exc_info=True)\n",
    "        continue\n",
    "\n",
    "    logger.info(\"enhance knowledge graph result: %s\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restful API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_name = \"Postman - Collections\"\n",
    "docs = client_docs[client_name]\n",
    "topic_name = docs[0]['topic_name']\n",
    "print(topic_name)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/upload\"\n",
    "\n",
    "for client_name in client_docs:\n",
    "    if client_name in [\n",
    "        \"Visa - AI Program Control Plane\",\n",
    "        \"Visa\",\n",
    "        \"Visa - Fast Data initiative\",\n",
    "        \"Visa VASPD\"\n",
    "    ]:\n",
    "        continue\n",
    "\n",
    "    docs = client_docs[client_name]\n",
    "    if len(docs) == 0:\n",
    "        print(f\"No docs for {client_name}\")\n",
    "        continue\n",
    "\n",
    "    topic_name = docs[0]['topic_name']\n",
    "    print(topic_name)\n",
    "    database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "\n",
    "    # Call the trigger-processing API to start processing uploaded all documents for a topic\n",
    "    url = \"http://192.168.206.252:23333/api/v1/knowledge/trigger-processing\"\n",
    "    data = {\n",
    "        \"topic_name\": topic_name,\n",
    "        \"database_uri\": database_uri\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=data)\n",
    "    print(response.status_code)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/upload\"\n",
    "\n",
    "files = []\n",
    "links = []\n",
    "for doc in docs:\n",
    "    files.append(('files', (doc[\"path\"].split('/')[-1], open(doc[\"path\"], 'rb'), 'application/pdf')))\n",
    "    links.append(doc[\"doc_link\"])\n",
    "\n",
    "data = {\n",
    "    'links': links,\n",
    "    'topic_name': topic_name,\n",
    "    'database_uri': os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "}\n",
    "response = requests.post(url, files=files, data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "\n",
    "# Call the trigger-processing API to start processing uploaded all documents for a topic\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/trigger-processing\"\n",
    "data = {\n",
    "    \"topic_name\": topic_name,\n",
    "    \"database_uri\": database_uri\n",
    "}\n",
    "\n",
    "response = requests.post(url, data=data)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Similarity based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph.query import search_relationships_by_vector_similarity, query_topic_graph\n",
    "\n",
    "query = \"Where are li ming now?\"\n",
    "res = search_relationships_by_vector_similarity(query, similarity_threshold=0.2, top_k=20)\n",
    "context = \"\"\n",
    "entities = set()\n",
    "relationships = []\n",
    "\n",
    "for index, row in res.iterrows():\n",
    "    entities.add(f\"{row['source_entity']} {row['source_entity_description']}\")\n",
    "    entities.add(f\"{row['target_entity']} {row['target_entity_description']}\")\n",
    "    relationships.append(f\"{row['source_entity']} {row['relationship_desc']} {row['target_entity']}\")\n",
    "\n",
    "context = \"Entities:\\n\" + \"\\n\".join(entities) + \"\\n\\nRelationships:\\n\" + \"\\n\".join(relationships)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.factory import LLMInterface\n",
    "\n",
    "llm_client = LLMInterface(\"bedrock\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "response =llm_client.generate(f\"\"\"Given the following context\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "answer the question: {query}\n",
    "\"\"\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
