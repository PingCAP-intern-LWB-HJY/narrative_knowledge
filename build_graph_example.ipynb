{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from llm.factory import LLMInterface\n",
    "from llm.embedding import get_text_embedding\n",
    "from setting.db import db_manager\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "from knowledge_graph.graph_builder import KnowledgeGraphBuilder\n",
    "\n",
    "llm_client = LLMInterface(\"ollama\", \"qwen3:32b-fp16\")\n",
    "kb_builder = KnowledgeBuilder()\n",
    "session_factory = db_manager.get_session_factory(os.getenv(\"GRAPH_DATABASE_URI\"))\n",
    "graph_builder = KnowledgeGraphBuilder(llm_client, get_text_embedding, session_factory)\n",
    "\n",
    "# Initialize logging module with a basic configuration for console output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s - %(filename)s:%(lineno)d: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "\n",
    "template_file_hashes = [\n",
    "    \"271bde1405442b4a2bb826eca673063e70833a4d02343e15ea9e46a00a218159\",\n",
    "    \"0b9400fec9aa5109d977699a65ce8dffce1290ee3f8ac2da4610078aa09d1d4b\",\n",
    "    \"9bf37d8935ab264a2308401cb9999e3d0d9d67a16c7f8adbb6a878ea607db9b4\",\n",
    "    \"c56bbc8c5345418191c9cf1081b592c368616c1f5d958712b45e2f0525a43352\",\n",
    "    \"c0cfa9e5942b48e58dc77226235dbac34c2d59ba0d3167434ecd682feb60e282\",\n",
    "    \"1c78dd8dd3f984f77bfc6f65d12fc7a949077f601b0a3995bd524c7f618d1407\",\n",
    "    \"641c93edb9df6d0325034bddd6ab1367be909616947798fde277425e441f26c9\",\n",
    "    \"d790176265d81503bdf5fe1a5070bb7507cb8b943801556c8aaa640629d27457\",\n",
    "    \"a57716c4f1806672cd79b92460fca79072faa5cc4840a933f688b4933756363d\",\n",
    "    \"6f4d6cf3926f3d098a8fd532787bff39e009451d42c29a4c25f9bfb79f48349b\",\n",
    "    \"0199cc4c09034b898dc8178d6cc4e97c4c1b0b99125e3396ca71040e2ce0df2f\",\n",
    "    \"8d382b2c935d733a0f0391b53c53ec34b96955b99f873a3d05af08452563ef05\",\n",
    "    \"f483e811a5ac4c15517ff5e660bfe17846d9daaa39863fb908d98b05a10bc069\",\n",
    "    \"57b42e65d8f935bec6dfc5e2f7ce9a00d58dcff06771003548bb627aeac56071\",\n",
    "    \"666010328f420ccbe6d383f62aa9db77a926bb06bf409811771ed9af3bf94da6\",\n",
    "    \"b3152523500b0e17540e5be3a2fa7c1bb4c2c48b4c5a926d8f6de71e7bbde1d0\"\n",
    "]\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = 'docs/pdf_metadata.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")\n",
    "\n",
    "\n",
    "client_name_list = set([doc['client_name'] for doc in loaded_docs])\n",
    "client_docs = {}\n",
    "for client_name in client_name_list:\n",
    "    client_docs[client_name] = []\n",
    "    for doc in loaded_docs:\n",
    "        with open(doc['path'], \"rb\") as f:\n",
    "            raw_content = f.read()\n",
    "            content_hash = hashlib.sha256(raw_content).hexdigest()\n",
    "            if content_hash in template_file_hashes:\n",
    "                continue\n",
    "        if doc['client_name'] == client_name:\n",
    "            client_docs[client_name].append({\n",
    "                'path': doc['path'],  # required\n",
    "                'doc_link': doc['web_view_link'], # required\n",
    "                'topic_name': f\"Customer Tracking for {doc['client_name']}\",\n",
    "                'client_name': doc['client_name'],\n",
    "                'created_time': doc['created_time'],\n",
    "                'modified_time': doc['modified_time'],\n",
    "                'mime_type': doc['mime_type']\n",
    "            })\n",
    "    print(f\"Client: {client_name}, Number of documents: {len(client_docs[client_name])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Client Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from knowledge_graph.models import SourceData\n",
    "from setting.db import db_manager\n",
    "\n",
    "def get_documents_by_topic(database_uri):\n",
    "    \"\"\"\n",
    "    Query database to get all documents aggregated by topic_name.\n",
    "    Returns a dict where each topic has an array of documents.\n",
    "    \"\"\"\n",
    "    topic_docs = defaultdict(list)\n",
    "    session_factory = db_manager.get_session_factory(database_uri)\n",
    "    with session_factory() as db:\n",
    "        # Query all source data\n",
    "        source_data_list = db.query(SourceData).all()\n",
    "        \n",
    "        for source_data in source_data_list:\n",
    "            # Extract topic_name from attributes\n",
    "            topic_name = None\n",
    "            if source_data.attributes and isinstance(source_data.attributes, dict):\n",
    "                topic_name = source_data.attributes.get('topic_name')\n",
    "            \n",
    "            # Skip if no topic_name found\n",
    "            if not topic_name:\n",
    "                continue\n",
    "                \n",
    "            # Add document to the appropriate topic\n",
    "            doc_info = {\n",
    "                \"source_id\": source_data.id,\n",
    "                \"source_name\": source_data.name,\n",
    "                \"source_content\": source_data.effective_content,\n",
    "                \"source_link\": source_data.link,\n",
    "                \"topic_name\": topic_name,\n",
    "                \"source_attributes\": source_data.attributes,\n",
    "            }\n",
    "            topic_docs[topic_name].append(doc_info)\n",
    "    \n",
    "    return dict(topic_docs)\n",
    "\n",
    "# Get all documents grouped by topic\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "all_topic_docs = get_documents_by_topic(database_uri)\n",
    "\n",
    "# Display available topics\n",
    "print(\"Available topics:\")\n",
    "for topic, docs in all_topic_docs.items():\n",
    "    print(f\"  {topic}: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = list(all_topic_docs.keys())\n",
    "topic_names = sorted(topic_names)\n",
    "for i, topic_name in enumerate(topic_names):\n",
    "    if i % 3 != 0:\n",
    "        continue\n",
    "    topic_docs = all_topic_docs[topic_name]\n",
    "    logger.info(\"processing topic: %s, number of docs: %d\", topic_name, len(topic_docs))\n",
    "    try:\n",
    "        result = graph_builder.build_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\n=== Memory Knowledge Graph Construction Results ===\")\n",
    "        logger.info(f\"Topic: {result['topic_name']}\")\n",
    "        logger.info(f\"Documents processed: {result['documents_processed']}\")\n",
    "        logger.info(f\"Documents failed: {result['documents_failed']}\")\n",
    "        logger.info(f\"Cognitive maps generated: {result['cognitive_maps_generated']}\")\n",
    "        logger.info(f\"Triplets extracted: {result['triplets_extracted']}\")\n",
    "        logger.info(f\"Total entities created: {result['entities_created']}\")\n",
    "        logger.info(f\"Total relationships created: {result['relationships_created']}\")\n",
    "\n",
    "        # Print global blueprint information\n",
    "        blueprint_info = result.get(\"global_blueprint\", {})\n",
    "        logger.info(f\"\\nGlobal Blueprint:\")\n",
    "        logger.info(\n",
    "            f\"  - Processing instructions: {blueprint_info.get('processing_instructions', '')}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"  - Processing items: {blueprint_info.get('processing_items', {})}\"\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\nðŸŽ‰ Memory knowledge graph construction completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build knowledge graph: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    try:\n",
    "        result = graph_builder.enhance_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ehance knowledge graph: {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "    logger.info(\"enhance knowledge graph result: %s\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name = \"Airbnb\"\n",
    "docs = client_docs[client_name]\n",
    "\n",
    "topic_name = f\"Customer Tracking for {client_name}\"\n",
    "\n",
    "print(\"step 1: upload docs to knowledge base\")\n",
    "topic_docs = {}\n",
    "for doc in docs:\n",
    "    file_path = doc['path']\n",
    "    try:\n",
    "        res = kb_builder.extract_knowledge(\n",
    "            file_path, \n",
    "            doc\n",
    "        )\n",
    "        if res['status'] == 'success':\n",
    "            topic_docs[res['source_id']] = {\n",
    "                \"source_id\": res['source_id'],\n",
    "                \"source_name\": res['source_name'],\n",
    "                \"source_content\": res['source_content'],\n",
    "                \"source_link\": res['source_link'],\n",
    "            } \n",
    "        else:\n",
    "            print(f\"process index {file_path} failed, {res['error']}\", exc_info=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"process index {file_path} failed, {e}\", exc_info=True)\n",
    "\n",
    "topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = f\"Customer {client_name} Tracking\"\n",
    "\n",
    "print(\"step 2: add to graph\")\n",
    "result = graph_builder.build_iterative_knowledge_graph(topic_name, list(topic_docs.values()))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restful API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name = \"Apple\"\n",
    "docs = client_docs[client_name]\n",
    "topic_name = docs[0]['topic_name']\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GRAPH_DATABASE_URI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# topic_name = f\"Customer Tracking for {client_name}\"\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/upload\"\n",
    "\n",
    "files = []\n",
    "links = []\n",
    "for doc in docs:\n",
    "    files.append(('files', (doc[\"path\"].split('/')[-1], open(doc[\"path\"], 'rb'), 'application/pdf')))\n",
    "    links.append(doc[\"doc_link\"])\n",
    "\n",
    "data = {\n",
    "    'links': links,\n",
    "    'topic_name': topic_name,\n",
    "    'database_uri': os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "}\n",
    "response = requests.post(url, files=files, data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "\n",
    "# Call the trigger-processing API to start processing uploaded all documents for a topic\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/trigger-processing\"\n",
    "data = {\n",
    "    \"topic_name\": topic_name,\n",
    "    \"database_uri\": database_uri\n",
    "}\n",
    "\n",
    "response = requests.post(url, data=data)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Similarity based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph.query import search_relationships_by_vector_similarity, query_topic_graph\n",
    "\n",
    "query = \"Where are li ming now?\"\n",
    "res = search_relationships_by_vector_similarity(query, similarity_threshold=0.2, top_k=20)\n",
    "context = \"\"\n",
    "entities = set()\n",
    "relationships = []\n",
    "\n",
    "for index, row in res.iterrows():\n",
    "    entities.add(f\"{row['source_entity']} {row['source_entity_description']}\")\n",
    "    entities.add(f\"{row['target_entity']} {row['target_entity_description']}\")\n",
    "    relationships.append(f\"{row['source_entity']} {row['relationship_desc']} {row['target_entity']}\")\n",
    "\n",
    "context = \"Entities:\\n\" + \"\\n\".join(entities) + \"\\n\\nRelationships:\\n\" + \"\\n\".join(relationships)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.factory import LLMInterface\n",
    "\n",
    "llm_client = LLMInterface(\"bedrock\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "response =llm_client.generate(f\"\"\"Given the following context\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "answer the question: {query}\n",
    "\"\"\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from knowledge_graph.query import query_topic_graph\n",
    "\n",
    "subgraph = query_topic_graph(\"Customer Tracking for Roblox\", os.getenv(\"GRAPH_DATABASE_URI\"))\n",
    "subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text  # Make sure to import text from sqlalchemy\n",
    "import pandas as pd\n",
    "import os\n",
    "from setting.db import db_manager\n",
    "\n",
    "def query_subgraph_by_source_hash(source_hash, database_uri=None):\n",
    "    \"\"\"\n",
    "    Query subgraph based on source document hash\n",
    "    \"\"\"\n",
    "    if database_uri is None:\n",
    "        database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "    \n",
    "    # Get database path\n",
    "    session_factory = db_manager.get_session_factory(database_uri)\n",
    "\n",
    "    query = text(\"\"\"\n",
    "    SELECT \n",
    "        e1.id as source_entity_id,\n",
    "        e1.name as source_entity_name,\n",
    "        e1.description as source_entity_description,\n",
    "        e1.attributes as source_entity_attributes,\n",
    "        e2.id as target_entity_id,\n",
    "        e2.name as target_entity_name, \n",
    "        e2.description as target_entity_description,\n",
    "        e2.attributes as target_entity_attributes,\n",
    "        r.id as relationship_id,\n",
    "        r.relationship_desc,\n",
    "        r.attributes as relationship_attributes\n",
    "    FROM relationships r\n",
    "    LEFT JOIN entities e1 ON r.source_entity_id = e1.id\n",
    "    LEFT JOIN entities e2 ON r.target_entity_id = e2.id\n",
    "    WHERE r.id IN (\n",
    "        SELECT graph_element_id \n",
    "        FROM source_graph_mapping \n",
    "        WHERE source_id IN (\n",
    "            SELECT id \n",
    "            FROM source_data \n",
    "            WHERE hash = :source_hash or content_hash = :source_hash\n",
    "        ) AND graph_element_type = 'relationship'\n",
    "    )\n",
    "    \"\"\")\n",
    "    \n",
    "    with session_factory() as session:\n",
    "        try:\n",
    "            # Fixed: Use dictionary for named parameters\n",
    "            result = session.execute(query, {\"source_hash\": source_hash})\n",
    "            # Fetch all results and convert to DataFrame\n",
    "            columns = result.keys()\n",
    "            rows = result.fetchall()\n",
    "            df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "            print(f\"Found {len(df)} relationships for source hash: {source_hash}\")\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error querying database: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "# Example usage - query relationships for a specific source document\n",
    "source_hash = \"33bab906ac24f6e02ccb1590b6b5bfb5c2ab528a2caee12037b7434a78add351\"\n",
    "subgraph_df = query_subgraph_by_source_hash(source_hash, os.getenv(\"GRAPH_DATABASE_URI\"))\n",
    "\n",
    "subgraph = {\n",
    "    \"entities\": {},\n",
    "    \"relationships\": {}\n",
    "}\n",
    "\n",
    "for index, row in subgraph_df.iterrows():\n",
    "    subgraph[\"entities\"][row['source_entity_id']] = {\n",
    "        \"id\": row[\"source_entity_id\"],\n",
    "        \"name\": row[\"source_entity_name\"],\n",
    "        \"description\": row[\"source_entity_description\"],\n",
    "        \"attributes\": row[\"source_entity_attributes\"]\n",
    "    }\n",
    "    subgraph[\"entities\"][row['target_entity_id']] = {\n",
    "        \"id\": row[\"target_entity_id\"],\n",
    "        \"name\": row[\"target_entity_name\"],\n",
    "        \"description\": row[\"target_entity_description\"],\n",
    "        \"attributes\": row[\"target_entity_attributes\"]\n",
    "    }\n",
    "    subgraph[\"relationships\"][row['relationship_id']] = {\n",
    "        \"id\": row[\"relationship_id\"],\n",
    "        \"source_entity\": row[\"source_entity_name\"],\n",
    "        \"target_entity\": row[\"target_entity_name\"],\n",
    "        \"relationship_desc\": row[\"relationship_desc\"],\n",
    "        \"attributes\": row[\"relationship_attributes\"]\n",
    "    }\n",
    "\n",
    "subgraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraphx = {\n",
    "    \"entities\": list(subgraph[\"entities\"].values()),\n",
    "    \"relationships\": list(subgraph[\"relationships\"].values())\n",
    "}\n",
    "\n",
    "subgraphx\n",
    "subgraph = subgraphx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subgraph[\"relationships\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "def visualize_knowledge_graph(subgraph_data, max_nodes=50, node_size_range=(1000, 3000)):\n",
    "    \"\"\"\n",
    "    Visualize knowledge graph with enhanced styling\n",
    "    \"\"\"\n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Extract entities and relationships\n",
    "    entities = subgraph_data.get('entities', [])\n",
    "    relationships = subgraph_data.get('relationships', [])\n",
    "    \n",
    "    # Limit number of nodes for better visualization\n",
    "    entities = entities[:max_nodes]\n",
    "    \n",
    "    # Create entity mapping\n",
    "    entity_map = {entity['name']: entity for entity in entities}\n",
    "    entity_names = set(entity_map.keys())\n",
    "    \n",
    "    # Add nodes\n",
    "    for entity in entities:\n",
    "        # Parse attributes\n",
    "        import json\n",
    "        try:\n",
    "            attrs = json.loads(entity['attributes'])\n",
    "            category = attrs.get('category', 'Unknown')\n",
    "            entity_type = attrs.get('entity_type', 'Unknown')\n",
    "        except:\n",
    "            category = 'Unknown'\n",
    "            entity_type = 'Unknown'\n",
    "            \n",
    "        G.add_node(entity['name'], \n",
    "                  category=category,\n",
    "                  entity_type=entity_type,\n",
    "                  description=entity['description'])\n",
    "    \n",
    "    # Add edges (only for entities that exist in our limited set)\n",
    "    edge_count = 0\n",
    "    for relationship in relationships:\n",
    "        source = relationship['source_entity']\n",
    "        target = relationship['target_entity']\n",
    "        \n",
    "        if source in entity_names and target in entity_names and edge_count < 100:\n",
    "            G.add_edge(source, target, \n",
    "                      relationship=relationship['relationship_desc'][:100] + \"...\" if len(relationship['relationship_desc']) > 100 else relationship['relationship_desc'])\n",
    "            edge_count += 1\n",
    "    \n",
    "    # Create figure with larger size\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    \n",
    "    # Use spring layout with better parameters\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50, seed=42)\n",
    "    \n",
    "    # Color mapping for categories\n",
    "    categories = list(set([G.nodes[node].get('entity_type', 'Unknown') for node in G.nodes()]))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(categories)))\n",
    "    category_colors = dict(zip(categories, colors))\n",
    "    \n",
    "    # Node colors based on category\n",
    "    node_colors = [category_colors.get(G.nodes[node].get('entity_type', 'Unknown'), 'lightblue') for node in G.nodes()]\n",
    "    \n",
    "    # Node sizes based on degree (connectivity)\n",
    "    degrees = dict(G.degree())\n",
    "    min_size, max_size = node_size_range\n",
    "    if degrees:\n",
    "        max_degree = max(degrees.values()) if degrees.values() else 1\n",
    "        min_degree = min(degrees.values()) if degrees.values() else 1\n",
    "        degree_range = max_degree - min_degree if max_degree > min_degree else 1\n",
    "        node_sizes = [min_size + (max_size - min_size) * (degrees.get(node, 1) - min_degree) / degree_range for node in G.nodes()]\n",
    "    else:\n",
    "        node_sizes = [min_size] * len(G.nodes())\n",
    "    \n",
    "    # Draw edges first (so they appear behind nodes)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', alpha=0.6, width=1, arrows=True, \n",
    "                          arrowsize=20, arrowstyle='->', connectionstyle='arc3,rad=0.1')\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, \n",
    "                          alpha=0.8, linewidths=2, edgecolors='black')\n",
    "    \n",
    "    # Add labels with better positioning\n",
    "    label_pos = {}\n",
    "    for node, (x, y) in pos.items():\n",
    "        # Wrap long labels\n",
    "        wrapped_label = '\\n'.join(textwrap.wrap(node, width=15))\n",
    "        label_pos[node] = wrapped_label\n",
    "    \n",
    "    nx.draw_networkx_labels(G, pos, labels=label_pos, font_size=8, font_weight='bold')\n",
    "    \n",
    "    # Create legend for categories\n",
    "    legend_elements = [plt.Rectangle((0,0),1,1, facecolor=category_colors[cat], \n",
    "                                   edgecolor='black', label=cat) for cat in categories[:10]]  # Limit legend items\n",
    "    \n",
    "    plt.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(1, 1), \n",
    "              title=\"Entity Categories\", title_fontsize=12)\n",
    "    \n",
    "    # Set title and remove axes\n",
    "    plt.title(f\"Apple Knowledge Graph Visualization\\n({len(G.nodes())} entities, {len(G.edges())} relationships)\", \n",
    "              fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Adjust layout to prevent legend cutoff\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add graph statistics\n",
    "    stats_text = f\"\"\"Graph Statistics:\n",
    "    â€¢ Nodes: {len(G.nodes())}\n",
    "    â€¢ Edges: {len(G.edges())}\n",
    "    â€¢ Avg Degree: {np.mean(list(degrees.values())):.2f}\n",
    "    â€¢ Max Degree: {max(degrees.values()) if degrees else 0}\n",
    "    â€¢ Connected Components: {nx.number_weakly_connected_components(G)}\"\"\"\n",
    "    \n",
    "    plt.figtext(0.02, 0.02, stats_text, fontsize=10, \n",
    "               bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print some analysis\n",
    "    print(f\"\\\\n=== Knowledge Graph Analysis ===\")\n",
    "    print(f\"Total entities: {len(entities)}\")\n",
    "    print(f\"Total relationships: {len(relationships)}\")\n",
    "    print(f\"Displayed entities: {len(G.nodes())}\")\n",
    "    print(f\"Displayed relationships: {len(G.edges())}\")\n",
    "    \n",
    "    if degrees:\n",
    "        # Find most connected entities\n",
    "        top_entities = sorted(degrees.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(f\"\\\\nMost connected entities:\")\n",
    "        for entity, degree in top_entities:\n",
    "            print(f\"  â€¢ {entity}: {degree} connections\")\n",
    "    \n",
    "    # Category distribution\n",
    "    category_count = defaultdict(int)\n",
    "    for entity in entities:\n",
    "        try:\n",
    "            attrs = json.loads(entity['attributes'])\n",
    "            category = attrs.get('category', 'Unknown')\n",
    "            category_count[category] += 1\n",
    "        except:\n",
    "            category_count['Unknown'] += 1\n",
    "    \n",
    "    print(f\"\\\\nEntity categories:\")\n",
    "    for category, count in sorted(category_count.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  â€¢ {category}: {count} entities\")\n",
    "    \n",
    "    return G\n",
    "\n",
    "# Visualize the subgraph\n",
    "G = visualize_knowledge_graph(subgraph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
