{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from llm.factory import LLMInterface\n",
    "from llm.embedding import get_text_embedding\n",
    "from setting.db import db_manager\n",
    "from knowledge_graph.knowledge import KnowledgeBuilder\n",
    "from knowledge_graph.graph_builder import KnowledgeGraphBuilder\n",
    "\n",
    "llm_client = LLMInterface(\"ollama\", \"qwen3:32b-fp16\")\n",
    "kb_builder = KnowledgeBuilder()\n",
    "session_factory = db_manager.get_session_factory(os.getenv(\"GRAPH_DATABASE_URI\"))\n",
    "graph_builder = KnowledgeGraphBuilder(llm_client, get_text_embedding, session_factory)\n",
    "\n",
    "# Initialize logging module with a basic configuration for console output\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(asctime)s] %(levelname)s - %(filename)s:%(lineno)d: %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import hashlib\n",
    "\n",
    "\n",
    "template_file_hashes = [\n",
    "    \"271bde1405442b4a2bb826eca673063e70833a4d02343e15ea9e46a00a218159\",\n",
    "    \"0b9400fec9aa5109d977699a65ce8dffce1290ee3f8ac2da4610078aa09d1d4b\",\n",
    "    \"9bf37d8935ab264a2308401cb9999e3d0d9d67a16c7f8adbb6a878ea607db9b4\",\n",
    "    \"c56bbc8c5345418191c9cf1081b592c368616c1f5d958712b45e2f0525a43352\",\n",
    "    \"c0cfa9e5942b48e58dc77226235dbac34c2d59ba0d3167434ecd682feb60e282\",\n",
    "    \"1c78dd8dd3f984f77bfc6f65d12fc7a949077f601b0a3995bd524c7f618d1407\",\n",
    "    \"641c93edb9df6d0325034bddd6ab1367be909616947798fde277425e441f26c9\",\n",
    "    \"d790176265d81503bdf5fe1a5070bb7507cb8b943801556c8aaa640629d27457\",\n",
    "    \"a57716c4f1806672cd79b92460fca79072faa5cc4840a933f688b4933756363d\",\n",
    "    \"6f4d6cf3926f3d098a8fd532787bff39e009451d42c29a4c25f9bfb79f48349b\",\n",
    "    \"0199cc4c09034b898dc8178d6cc4e97c4c1b0b99125e3396ca71040e2ce0df2f\",\n",
    "    \"8d382b2c935d733a0f0391b53c53ec34b96955b99f873a3d05af08452563ef05\",\n",
    "    \"f483e811a5ac4c15517ff5e660bfe17846d9daaa39863fb908d98b05a10bc069\",\n",
    "    \"57b42e65d8f935bec6dfc5e2f7ce9a00d58dcff06771003548bb627aeac56071\",\n",
    "    \"666010328f420ccbe6d383f62aa9db77a926bb06bf409811771ed9af3bf94da6\",\n",
    "    \"b3152523500b0e17540e5be3a2fa7c1bb4c2c48b4c5a926d8f6de71e7bbde1d0\"\n",
    "]\n",
    "\n",
    "# Define the path to the JSON configuration file\n",
    "config_file_path = 'docs/pdf_metadata.json'\n",
    "\n",
    "# Variable to store the loaded data\n",
    "loaded_docs = []\n",
    "\n",
    "# Read the JSON configuration file\n",
    "try:\n",
    "    with open(config_file_path, 'r', encoding='utf-8') as f:\n",
    "        loaded_docs = json.load(f)\n",
    "    print(f\"Successfully loaded configuration from: {config_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Configuration file not found at '{config_file_path}'\")\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from file '{config_file_path}'. Check file format.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while reading the file: {e}\")\n",
    "\n",
    "if len(loaded_docs) > 0:\n",
    "    print(\"\\nExample: Accessing first document data:\")\n",
    "    print(loaded_docs[0])\n",
    "else:\n",
    "    print(\"\\nConfiguration file is empty.\")\n",
    "\n",
    "\n",
    "client_name_list = set([doc['client_name'] for doc in loaded_docs])\n",
    "client_docs = {}\n",
    "for client_name in client_name_list:\n",
    "    client_docs[client_name] = []\n",
    "    for doc in loaded_docs:\n",
    "        with open(doc['path'], \"rb\") as f:\n",
    "            raw_content = f.read()\n",
    "            content_hash = hashlib.sha256(raw_content).hexdigest()\n",
    "            if content_hash in template_file_hashes:\n",
    "                continue\n",
    "        if doc['client_name'] == client_name:\n",
    "            client_docs[client_name].append({\n",
    "                'path': doc['path'],  # required\n",
    "                'doc_link': doc['web_view_link'], # required\n",
    "                'topic_name': f\"Customer Tracking for {doc['client_name']}\",\n",
    "                'client_name': doc['client_name'],\n",
    "                'created_at': doc['created_time'],\n",
    "                'updated_at': doc['modified_time'],\n",
    "                'mime_type': doc['mime_type']\n",
    "            })\n",
    "    print(f\"Client: {client_name}, Number of documents: {len(client_docs[client_name])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data and Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_name = \"OpenAI\"\n",
    "docs = client_docs[client_name]\n",
    "topic_name = f\"Customer Tracking for {client_name}\"\n",
    "\n",
    "print(\"step 1: upload docs to knowledge base\")\n",
    "topic_docs = {}\n",
    "for doc in docs:\n",
    "    file_path = doc['path']\n",
    "    try:\n",
    "        res = kb_builder.extract_knowledge(\n",
    "            file_path, \n",
    "            doc\n",
    "        )\n",
    "        if res['status'] == 'success':\n",
    "            topic_docs[res['source_id']] = {\n",
    "                \"source_id\": res['source_id'],\n",
    "                \"source_name\": res['source_name'],\n",
    "                \"source_content\": res['source_content'],\n",
    "                \"source_link\": res['source_link'],\n",
    "            } \n",
    "        else:\n",
    "            print(f\"process index {file_path} failed, {res['error']}\", exc_info=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"process index {file_path} failed, {e}\", exc_info=True)\n",
    "\n",
    "topic_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = f\"Customer {client_name} Tracking\"\n",
    "\n",
    "print(\"step 2: add to graph\")\n",
    "result = graph_builder.build_knowledge_graph(topic_name, list(topic_docs.values()))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Build Graph \n",
    "\n",
    "Assuming that source data already uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from knowledge_graph.models import SourceData\n",
    "from setting.db import db_manager\n",
    "\n",
    "def get_documents_by_topic(database_uri):\n",
    "    \"\"\"\n",
    "    Query database to get all documents aggregated by topic_name.\n",
    "    Returns a dict where each topic has an array of documents.\n",
    "    \"\"\"\n",
    "    topic_docs = defaultdict(list)\n",
    "    session_factory = db_manager.get_session_factory(database_uri)\n",
    "    with session_factory() as db:\n",
    "        # Query all source data\n",
    "        source_data_list = db.query(SourceData).all()\n",
    "        \n",
    "        for source_data in source_data_list:\n",
    "            # Extract topic_name from attributes\n",
    "            topic_name = None\n",
    "            if source_data.attributes and isinstance(source_data.attributes, dict):\n",
    "                topic_name = source_data.attributes.get('topic_name')\n",
    "            \n",
    "            # Skip if no topic_name found\n",
    "            if not topic_name:\n",
    "                continue\n",
    "                \n",
    "            # Add document to the appropriate topic\n",
    "            doc_info = {\n",
    "                \"source_id\": source_data.id,\n",
    "                \"source_name\": source_data.name,\n",
    "                \"source_content\": source_data.effective_content,\n",
    "                \"source_link\": source_data.link,\n",
    "                \"topic_name\": topic_name,\n",
    "                \"source_attributes\": source_data.attributes,\n",
    "            }\n",
    "            topic_docs[topic_name].append(doc_info)\n",
    "    \n",
    "    return dict(topic_docs)\n",
    "\n",
    "# Get all documents grouped by topic\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "all_topic_docs = get_documents_by_topic(database_uri)\n",
    "\n",
    "# Display available topics\n",
    "print(\"Available topics:\")\n",
    "for topic, docs in all_topic_docs.items():\n",
    "    print(f\"  {topic}: {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = list(all_topic_docs.keys())\n",
    "topic_names = sorted(topic_names)\n",
    "for i, topic_name in enumerate(topic_names):\n",
    "    if i % 6 != 4:\n",
    "        continue\n",
    "    topic_docs = all_topic_docs[topic_name]\n",
    "    logger.info(\"processing topic: %s, number of docs: %d\", topic_name, len(topic_docs))\n",
    "    try:\n",
    "        result = graph_builder.build_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\n=== Memory Knowledge Graph Construction Results ===\")\n",
    "        logger.info(f\"Topic: {result['topic_name']}\")\n",
    "        logger.info(f\"Documents processed: {result['documents_processed']}\")\n",
    "        logger.info(f\"Documents failed: {result['documents_failed']}\")\n",
    "        logger.info(f\"Cognitive maps generated: {result['cognitive_maps_generated']}\")\n",
    "        logger.info(f\"Triplets extracted: {result['triplets_extracted']}\")\n",
    "        logger.info(f\"Total entities created: {result['entities_created']}\")\n",
    "        logger.info(f\"Total relationships created: {result['relationships_created']}\")\n",
    "\n",
    "        # Print global blueprint information\n",
    "        blueprint_info = result.get(\"global_blueprint\", {})\n",
    "        logger.info(f\"\\nGlobal Blueprint:\")\n",
    "        logger.info(\n",
    "            f\"  - Processing instructions: {blueprint_info.get('processing_instructions', '')}\"\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"  - Processing items: {blueprint_info.get('processing_items', {})}\"\n",
    "        )\n",
    "\n",
    "        logger.info(\"\\nðŸŽ‰ Memory knowledge graph construction completed successfully!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to build knowledge graph: {e}\", exc_info=True)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        result = graph_builder.enhance_knowledge_graph(\n",
    "            topic_name,\n",
    "            topic_docs,\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to ehance knowledge graph: {e}\", exc_info=True)\n",
    "        continue\n",
    "\n",
    "    logger.info(\"enhance knowledge graph result: %s\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restful API Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client_name = \"Postman - Collections\"\n",
    "docs = client_docs[client_name]\n",
    "topic_name = docs[0]['topic_name']\n",
    "print(topic_name)\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/upload\"\n",
    "\n",
    "for client_name in client_docs:\n",
    "    if client_name in [\n",
    "        \"Visa - AI Program Control Plane\",\n",
    "        \"Visa\",\n",
    "        \"Visa - Fast Data initiative\",\n",
    "        \"Visa VASPD\"\n",
    "    ]:\n",
    "        continue\n",
    "\n",
    "    docs = client_docs[client_name]\n",
    "    if len(docs) == 0:\n",
    "        print(f\"No docs for {client_name}\")\n",
    "        continue\n",
    "\n",
    "    topic_name = docs[0]['topic_name']\n",
    "    print(topic_name)\n",
    "    database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "\n",
    "    # Call the trigger-processing API to start processing uploaded all documents for a topic\n",
    "    url = \"http://192.168.206.252:23333/api/v1/knowledge/trigger-processing\"\n",
    "    data = {\n",
    "        \"topic_name\": topic_name,\n",
    "        \"database_uri\": database_uri\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, data=data)\n",
    "    print(response.status_code)\n",
    "    print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/upload\"\n",
    "\n",
    "files = []\n",
    "links = []\n",
    "for doc in docs:\n",
    "    files.append(('files', (doc[\"path\"].split('/')[-1], open(doc[\"path\"], 'rb'), 'application/pdf')))\n",
    "    links.append(doc[\"doc_link\"])\n",
    "\n",
    "data = {\n",
    "    'links': links,\n",
    "    'topic_name': topic_name,\n",
    "    'database_uri': os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "}\n",
    "response = requests.post(url, files=files, data=data)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "database_uri = os.getenv(\"GRAPH_DATABASE_URI\")\n",
    "\n",
    "# Call the trigger-processing API to start processing uploaded all documents for a topic\n",
    "url = \"http://192.168.206.252:23333/api/v1/knowledge/trigger-processing\"\n",
    "data = {\n",
    "    \"topic_name\": topic_name,\n",
    "    \"database_uri\": database_uri\n",
    "}\n",
    "\n",
    "response = requests.post(url, data=data)\n",
    "print(response.status_code)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Similarity based Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_graph.query import search_relationships_by_vector_similarity, query_topic_graph\n",
    "\n",
    "query = \"Where are li ming now?\"\n",
    "res = search_relationships_by_vector_similarity(query, similarity_threshold=0.2, top_k=20)\n",
    "context = \"\"\n",
    "entities = set()\n",
    "relationships = []\n",
    "\n",
    "for index, row in res.iterrows():\n",
    "    entities.add(f\"{row['source_entity']} {row['source_entity_description']}\")\n",
    "    entities.add(f\"{row['target_entity']} {row['target_entity_description']}\")\n",
    "    relationships.append(f\"{row['source_entity']} {row['relationship_desc']} {row['target_entity']}\")\n",
    "\n",
    "context = \"Entities:\\n\" + \"\\n\".join(entities) + \"\\n\\nRelationships:\\n\" + \"\\n\".join(relationships)\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.factory import LLMInterface\n",
    "\n",
    "llm_client = LLMInterface(\"bedrock\", \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\")\n",
    "response =llm_client.generate(f\"\"\"Given the following context\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "answer the question: {query}\n",
    "\"\"\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
